```{r}
n <- 200 # number of datapoints
p <- 2 # dimension
sigma <- 1 # variance of the distribution
mpos <- 0 # mean value ( centre of the distribution) of positive examples
mneg <- 3 # mean valuecentre of the distribution) of negative examples
npos <- round (n / 2) # number of positive examples
nneg <- n - npos # number of negative examples
# Generate the positive and negative examples
xpos <- matrix ( rnorm ( npos * p , mean = mpos , sd = sigma ), npos , p)
xneg <- matrix ( rnorm ( nneg * p , mean = mneg , sd = sigma ), nneg , p)
x <- rbind ( xpos , xneg )
# Generate labels
y <- matrix (c( rep (1 , npos ), rep ( -1 , nneg )))
# Visualize the data
plot (x , col = ifelse (y > 0, 1, 2))
legend ("topleft" , c( 'Positive ', 'Negative'), col = seq (2) , pch = 1,
        text.col = seq (2))
## Prepare a training and a test set ##
ntrain <- round (n * 0.8) # number of training examples
tindex <- sample (n , ntrain ) # indices of training samples
XT <- x[ tindex ,] # positive training examples
XX <- x[- tindex ,] # positive test examples
YT <- y[ tindex ] # negative training examples
YY <- y[- tindex ] # negative test examples
istrain = rep (0 , n)
istrain [ tindex ] = 1
# Visualize
plot (x , col = ifelse (y > 0 ,1 ,2) , pch = ifelse ( istrain == 1, 1, 2))
legend ("topleft" , c( 'Positive Train', 'Positive Test', 'Negative Train',
'Negative Test'), col=c (1 , 1, 2, 2) , pch = c (1 , 2, 1, 2) ,
text.col = c(1 , 1, 2, 2))
```

```{r}
# Load the kernlab package
library ( kernlab )
# Train the SVM
svp <- ksvm (XT , YT , type = "C-svc" , kernel = 'vanilladot', C = 100 , scaled=c())
```

```{r}
# General summary
svp
# Attributes that you can access
attributes(svp)
# For example , the support vectors
alpha (svp)
alphaindex(svp)
b(svp)
# Use the built - in function to pretty - plot the classifier
plot (svp, data = XT)
```


2.
Number of Attributes german: 20 (7 numerical, 13 categorical)
Number of Attributes german.numer: 24 (24 numerical)

There is two class defined good and bad

```{r}
german = read.csv(file = 'german.csv', header = TRUE)

dim(german)
nrow(german)
head(german)
tail(german)
summary(german)
names(german)
str(german)
```
```{r}
color <- c("red","green", "blue")
print(color)
```

```{r}
class(german)
dim(german)
class(german$Job)
sapply(german, class)
```

First Part at 5
```{r}
german$Class <- factor(german$Class)
```

```{r}
class(german[1,])
dim(german[1,])
```

```{r}
class(german[c(1,3),])
dim(german[c(1,3),])
german[c(1,3),]
```

```{r}
class(german$Age)
dim(german$Age)
german$Age
```

```{r}
class(german[1])
dim(german[1])

```

```{r}
class(german["Age"])
dim(german["Age"])
german["Age"]
```

```{r}
class(german[,c(1,3)])
dim(german[,c(1,3)])
```

```{r}
class(german[,c("Age","Job")])
dim(german[,c("Age","Job")])
```

```{r}
class(german[3,c("Age","Job")])
dim(german[3,c("Age","Job")])

german[3,c("Age","Job")]
```

```{r}
class(german[which(german$Age>70),])
dim(german[which(german$Age>70),])
german[which(german$Age>70),]
```

```{r}
class(german[which(german$Job=="A171"),])
dim(german[which(german$Job=="A171"),])
```

```{r}
class(german[-1,])
dim(german[-1,])
german[-1,]
```

```{r}
class(german[-c(3,6),])
dim(german[-c(3,6),])
```

Seconpqrt Example:
```{r}
table(german$Job) / nrow(german)
```

```{r}
mean(german$Age)
sd(german$Age)
```


```{r}
#tutorial followed https://machinelearningmastery.com/machine-learning-in-r-step-by-step/
#Frenquency distribution
table(german$Class)
table(german$Job)
#Relative distribution
table(german$Class)/nrow(german)
table(german$Job)/nrow(german)
#Mean of Age
mean(german$Age)
#Standard deviation of Age
sd(german$Age)


#Class Frenquency distribution
table(german$Class)
#Class relative distribution
table(german$Class)/nrow(german)
#Mean of Class
mean(table(german$Class))
#Standard deviation of Class
sd(table(german$Class))

#Accuracy of a classification model

# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(german$Class, p=0.80, list=FALSE)
# select 20% of the data for validation
validation <- german[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- german[validation_index,]

# dimensions of dataset
dim(dataset)
# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
levels(dataset$Class)

# summarize the class distribution
percentage <- prop.table(table(dataset$Class)) * 100
cbind(freq=table(dataset$Class), percentage=percentage)

# summarize the class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq=table(dataset$Species), percentage=percentage)

# summarize attribute distributions
# summary(dataset)

# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"

# a) linear algorithms
set.seed(7)
fit.lda <- train(Class~., data=dataset, method="lda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- train(Class~., data=dataset, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- train(Class~., data=dataset, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- train(Class~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(Class~., data=dataset, method="rf", metric=metric, trControl=control)

# summarize accuracy of models
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)

# compare accuracy of models
dotplot(results)

# summarize Best Model
print(fit.lda)

# estimate skill of LDA on the validation dataset
predictions <- predict(fit.lda, validation)
confusionMatrix(predictions, validation$Class)

```
5) Second part
```{r}
ggplot(german,aes(x=german$Property,fill=german$Class)) + geom_bar(position="dodge")
```

```{r}
ggplot(german,aes(x=german$Age)) + geom_density()
```


```{r}
#7. cor funciton

class(german$Duration)
class(german$Age)
class(german$CreditAmount)
print("Cor between Duration and Age")
cor(german$Duration,german$Age)
print("Cor between Duration and CreditAmount")
cor(german$Duration,german$CreditAmount)
print("Cor between Age and CreditAmount")
cor(german$Age,german$CreditAmount)
```

```{r}
#8. page 4
print(chisq.test(german$Job,german$PersonalStatus))
```

```{r}
#9. 
#With Age
oneway.test(german$Age ~ german$Class,german)
#With Duration
oneway.test(german$Duration ~ german$Class,german)
#With PresentResidenceSince
oneway.test(german$PresentResidenceSince ~ german$Class,german)
#With CreditAmount
oneway.test(german$CreditAmount ~ german$Class,german)
#With InstallmentRate
oneway.test(german$InstallmentRate ~ german$Class,german)


```

```{r}
# Example
# index <- sample(1:nrow(german),size = 0.1* nrow(german))
# test <- german[index,]
# train <- german[-index,]

# Split between 90 and 10
index <- sample(1:nrow(german),size = 0.9* nrow(german))
# Use the large partition to training - 90
train <- german[index,]
# Use the small partitio to test - 10
test <- german[-index,]
```

```{r}
#Tree traing
t1 = rpart(Class ~., data = train)
print(t1)


```
```{r}
#Tree test
t1_test = rpart(Class ~., data = test)
print(t1_test)
```

```{r}
prp(t1, extra=101)
```
```{r}
prp(t1_test, extra=101)
```

```{r}
#4.
pred <- predict(t1, train, type = "class")
mConfusion <- table(train$Class, pred)
TrainErr <- 1 - (mConfusion[1,1]+mConfusion[2,2])/sum(mConfusion)
print(TrainErr)
#5. Compute errors
TrainErr5 <- 1 - (mConfusion[1,2]+mConfusion[2,1])/sum(mConfusion)
print(TrainErr5)
```
```{r}
pred_test <- predict(t1, test, type = "class")
mConfusion_test <- table(test$Class, pred_test)
TrainErr_test <- 1 - (mConfusion_test[1,1]+mConfusion_test[2,2])/sum(mConfusion_test)
print(TrainErr_test)
#5. Compute errors
TrainErr5_test <- 1 - (mConfusion_test[1,2]+mConfusion_test[2,1])/sum(mConfusion_test)
print(TrainErr5_test)
```


```{r}
#6. 
pred <- predict(t1, train, type = "prob")
dpred <- as.data.frame(pred)
mpred <- melt(dpred, measure.vars = 1:2)
ggplot(mpred, aes(x=mpred$value)) + geom_freqpoly(aes(x=value,colours= variable),bins =50)

print(pred)
print(mpred)

melt(dpred, measure.vars = 1:2)
```

```{r}
print(dpred[1])

mpred1 <- melt(dpred, measure.vars = 1:1)
ggplot(mpred1, aes(x=mpred$value)) + geom_freqpoly(aes(x=value,colours= variable),bins =50)
```

```{r}
mpred1 <- melt(dpred, measure.vars = 2:2)
ggplot(mpred1, aes(x=mpred$value)) + geom_freqpoly(aes(x=value,colours= variable),bins =50)
```


```{r}
#Pruning a decision tree train
#1

t2 <- rpart(Class ~., data = train, control = rpart.control(minsplit=10,cp=0))
print(t2)
```

```{r}
prp(t2, extra=101)
```

```{r}
predT2 <- predict(t2, train, type = "class")

mConfusionT2 <- table(train$Class, predT2)
TrainErrT2 <- 1 - (mConfusionT2[1,1]+mConfusionT2[2,2])/sum(mConfusionT2)

print("Tree 2 for error of training")
print(TrainErrT2)
```

```{r}
#Tree t2 training 
printcp(t2)
```

```{r}
plotcp(t2)
```


```{r}
#Pruning a decision tree train
#1

t2_test <- rpart(Class ~., data = test, control = rpart.control(minsplit=10,cp=0))
print(t2_test)
```

```{r}
#Tree t2 test 
printcp(t2_test)
```

```{r}
prp(t2_test, extra=101)
```


```{r}
predT2_test <- predict(t2, test, type = "class")

mConfusionT2_test <- table(test$Class, predT2_test)
TrainErrT2_test <- 1 - (mConfusionT2_test[1,1]+mConfusionT2_test[2,2])/sum(mConfusionT2_test)
print("Tree 2 for error of test")
print(TrainErrT2_test)

```

```{r}
t2p <- prune(t2, cp = 0.035714)
print(t2p)
print(t2)
```

